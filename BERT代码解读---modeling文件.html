<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>BERT代码解读---modeling文件</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1><a id="BERTmodeling_0"></a>BERT代码解读—modeling文件</h1>
<p>前几天没写博客，一是主要在对之前的一些工作进一步理解，并且因为生病耽误了一两天。前两天弄好了BERT的模型，在自己的下层任务已经能够运行起来了，但是速度实在是慢，考虑学习一下使用google colab，在上面跑一下看看。今天先学习一下的BERT整体代码，因为之前fine tune 只修改了部分文件的代码。由于代码实在太长了，决定每个文件分开写。下面是modeling文件。<br>
<br></p>
<blockquote>
<p>参考：https://daiwk.github.io/posts/nlp-bert-code-annotated-framework.html  bert代码解读——framework<br>
参考：https://www.jianshu.com/p/d7ce41b58801	Bert系列（二）——源码解读之模型主体</p>
</blockquote>
<br>
<h1><a id="modeling_9"></a>modeling文件</h1>
<p>主要三个部分，公共函数，BertConfig类和BertModel类。</p>
<h2><a id="_11"></a>公共函数：</h2>
<h4><a id="glue_12"></a>glue:</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">gelu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Gaussian Error Linear Unit.

  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    x: float Tensor to perform activation.

  Returns:
    `x` with the GELU activation applied.
  """</span>
  cdf <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>
      <span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">0.044715</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> x <span class="token operator">*</span> cdf
</code></pre>
<p>glue是激活函数，Bert模型中没有使用relu函数而使用了gelu。<br>
这里顺便说一下激活函数：<br>
Relu：<br>
<img src="https://img-blog.csdnimg.cn/20190704094259189.png" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20190704094253597.png" alt="在这里插入图片描述"><br>
  优点：解决了饱和的问题，计算快(sigmoid 和 tanh需要计算指数)。<br>
  缺点：负数无法激活。缺乏随机因素。<br>
Elu：<br>
<img src="https://img-blog.csdnimg.cn/20190704094309265.png" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20190704094318806.png" alt="在这里插入图片描述"><br>
  相比Relu解决了负区域不激活的问题，不过还是有饱和以及指数运算的问题。<br>
PRelu：<br>
<img src="https://img-blog.csdnimg.cn/20190704094500625.png" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/2019070409450646.png" alt="在这里插入图片描述"><br>
alpha一般比较小，当alpha=0.01时，PRelu为Leaky Relu。<br>
GELUS：</p>
<blockquote>
<p>论文地址：https://arxiv.org/abs/1606.08415  Gaussian Error Linear Units (GELUs)<br>
<img src="https://img-blog.csdnimg.cn/20190704102531779.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
随着x增大，P也会接近于1，Gelu的斜率接近1。随着x的减小，P也会接近0，最终越小Gelu的斜率接近0。相比于relu，增加了统计特性，论文证明在多个深度学习任务中都优于relu的效果。<br>
<img src="https://img-blog.csdnimg.cn/20190704101358335.png" alt="在这里插入图片描述"><br>
  在激活函数中引入了随机正则的变化，x是输入，X服从标准正太分布，这样就通过P(X&lt;=x)来决定保留x的信息的比例，当x减小时，就有更高的概率减少信息。这样就能够随机依赖输入。这里可以使用标准正太分布，也可以使用参数，通过训练得到μ和σ。下面写一个计算正太分布的例子。<br>
<img src="https://img-blog.csdnimg.cn/20190704102205893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
<p>最后，论文提供了GELU(x)近似计算的数学公式，BERT代码就是这个公式：<br>
<img src="https://img-blog.csdnimg.cn/20190704103058224.png" alt="在这里插入图片描述"></p>
<h4><a id="get_activation_54"></a>get_activation</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_activation</span><span class="token punctuation">(</span>activation_string<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Maps a string to a Python function, e.g., "relu" =&gt; `tf.nn.relu`.

  Args:
    activation_string: String name of the activation function.

  Returns:
    A Python function corresponding to the activation function. If
    `activation_string` is None, empty, or "linear", this will return None.
    If `activation_string` is not a string, it will return `activation_string`.

  Raises:
    ValueError: The `activation_string` does not correspond to a known
      activation.
  """</span>

  <span class="token comment"># We assume that anything that"s not a string is already an activation</span>
  <span class="token comment"># function, so we just return it.</span>
  <span class="token keyword">if</span> <span class="token operator">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>activation_string<span class="token punctuation">,</span> six<span class="token punctuation">.</span>string_types<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> activation_string

  <span class="token keyword">if</span> <span class="token operator">not</span> activation_string<span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token boolean">None</span>

  act <span class="token operator">=</span> activation_string<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token keyword">if</span> act <span class="token operator">==</span> <span class="token string">"linear"</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token boolean">None</span>
  <span class="token keyword">elif</span> act <span class="token operator">==</span> <span class="token string">"relu"</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu
  <span class="token keyword">elif</span> act <span class="token operator">==</span> <span class="token string">"gelu"</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> gelu
  <span class="token keyword">elif</span> act <span class="token operator">==</span> <span class="token string">"tanh"</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>tanh
  <span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Unsupported activation: %s"</span> <span class="token operator">%</span> act<span class="token punctuation">)</span>
</code></pre>
<p>把输入的字符串映射为python的函数。<br>
<br></p>
<h4><a id="get_assignment_map_from_checkpoint_95"></a>get_assignment_map_from_checkpoint</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_assignment_map_from_checkpoint</span><span class="token punctuation">(</span>tvars<span class="token punctuation">,</span> init_checkpoint<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Compute the union of the current variables and checkpoint variables."""</span>
  <span class="token comment"># 计算当前变量和cheeckpoint中变量的并集</span>
  assignment_map <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  initialized_variable_names <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

  name_to_variable <span class="token operator">=</span> collections<span class="token punctuation">.</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token keyword">for</span> var <span class="token keyword">in</span> tvars<span class="token punctuation">:</span>
    name <span class="token operator">=</span> var<span class="token punctuation">.</span>name
    m <span class="token operator">=</span> re<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token string">"^(.*):\\d+$"</span><span class="token punctuation">,</span> name<span class="token punctuation">)</span>
    <span class="token keyword">if</span> m <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
      name <span class="token operator">=</span> m<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    name_to_variable<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> var

  init_vars <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>list_variables<span class="token punctuation">(</span>init_checkpoint<span class="token punctuation">)</span>
  <span class="token comment"># 返回checkpoint中的所有变量   tuple(name, shape)</span>

  assignment_map <span class="token operator">=</span> collections<span class="token punctuation">.</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token keyword">for</span> x <span class="token keyword">in</span> init_vars<span class="token punctuation">:</span>
    <span class="token punctuation">(</span>name<span class="token punctuation">,</span> var<span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> name <span class="token operator">not</span> <span class="token keyword">in</span> name_to_variable<span class="token punctuation">:</span>
      <span class="token keyword">continue</span>
    assignment_map<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> name
    initialized_variable_names<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
    initialized_variable_names<span class="token punctuation">[</span>name <span class="token operator">+</span> <span class="token string">":0"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>

  <span class="token keyword">return</span> <span class="token punctuation">(</span>assignment_map<span class="token punctuation">,</span> initialized_variable_names<span class="token punctuation">)</span>
</code></pre>
<p>从checkpoint的list_variables中，获取tvars（一般是tvars = tf.trainable_variables()）中的变量<br>
<br></p>
<h4><a id="dropout_128"></a>dropout</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">dropout</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> dropout_prob<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Perform dropout.

  Args:
    input_tensor: float Tensor.
    dropout_prob: Python float. The probability of dropping out a value (NOT of
      *keeping* a dimension as in `tf.nn.dropout`).

  Returns:
    A version of `input_tensor` with dropout applied.
  """</span>
  <span class="token keyword">if</span> dropout_prob <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token operator">or</span> dropout_prob <span class="token operator">==</span> <span class="token number">0.0</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> input_tensor

  output <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> <span class="token number">1.0</span> <span class="token operator">-</span> dropout_prob<span class="token punctuation">)</span>
  <span class="token keyword">return</span> output
</code></pre>
<br>
<h4><a id="layer_norm_150"></a>layer_norm</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">layer_norm</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Run layer normalization on the last dimension of the tensor."""</span>
  <span class="token keyword">return</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>
      inputs<span class="token operator">=</span>input_tensor<span class="token punctuation">,</span> begin_norm_axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> begin_params_axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> scope<span class="token operator">=</span>name<span class="token punctuation">)</span>
</code></pre>
<p>Layer Normalization与Batch Normalization的不同在于：BatchNorm是在单个神经元的batch上做规则化，LayerNorm是对层的神经元做规则化方向上。<br>
<br></p>
<h4><a id="layer_norm_and_dropout_160"></a>layer_norm_and_dropout</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">layer_norm_and_dropout</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> dropout_prob<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Runs layer normalization followed by dropout."""</span>
  output_tensor <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> name<span class="token punctuation">)</span>
  output_tensor <span class="token operator">=</span> dropout<span class="token punctuation">(</span>output_tensor<span class="token punctuation">,</span> dropout_prob<span class="token punctuation">)</span>
  <span class="token keyword">return</span> output_tensor
</code></pre>
<p>调用LN和dropout。<br>
<br></p>
<h4><a id="def_create_initializer_171"></a>def create_initializer</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">create_initializer</span><span class="token punctuation">(</span>initializer_range<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Creates a `truncated_normal_initializer` with the given range."""</span>
  <span class="token keyword">return</span> tf<span class="token punctuation">.</span>truncated_normal_initializer<span class="token punctuation">(</span>stddev<span class="token operator">=</span>initializer_range<span class="token punctuation">)</span>
</code></pre>
<p>初始化。<br>
<br></p>
<h4><a id="embedding_lookup_180"></a>embedding_lookup</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">embedding_lookup</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span>
                     vocab_size<span class="token punctuation">,</span>
                     embedding_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>
                     initializer_range<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">,</span>
                     word_embedding_name<span class="token operator">=</span><span class="token string">"word_embeddings"</span><span class="token punctuation">,</span>
                     use_one_hot_embeddings<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Looks up words embeddings for id tensor.

  Args:
    input_ids: shape为包含了word ids的[batch_size, seq_length]的tensor
      ids.即batch行，seq列，每个位置都是id。
    vocab_size: 字典的size，embedding的行长度。
    embedding_size: embedding 维度。
    initializer_range: float. Embedding initialization range.
    word_embedding_name: string. Name of the embedding table.
    use_one_hot_embeddings: bool. If True, use one-hot method for word
      embeddings. If False, use `tf.gather()`.

  Returns:
    float Tensor of shape [batch_size, seq_length, embedding_size].
    返回输入的shape的对应的embedding表示。以及完整的table_embedding。
  """</span>
  <span class="token comment"># This function assumes that the input is of shape [batch_size, seq_length,</span>
  <span class="token comment"># num_inputs].</span>
  <span class="token comment">#</span>
  <span class="token comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span>
  <span class="token comment"># reshape to [batch_size, seq_length, 1].</span>
  <span class="token keyword">if</span> input_ids<span class="token punctuation">.</span>shape<span class="token punctuation">.</span>ndims <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
    input_ids <span class="token operator">=</span> tf<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  embedding_table <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span>
      name<span class="token operator">=</span>word_embedding_name<span class="token punctuation">,</span>
      shape<span class="token operator">=</span><span class="token punctuation">[</span>vocab_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">]</span><span class="token punctuation">,</span>
      initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>

  flat_input_ids <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
  <span class="token keyword">if</span> use_one_hot_embeddings<span class="token punctuation">:</span>
    one_hot_input_ids <span class="token operator">=</span> tf<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>flat_input_ids<span class="token punctuation">,</span> depth<span class="token operator">=</span>vocab_size<span class="token punctuation">)</span>
    output <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>one_hot_input_ids<span class="token punctuation">,</span> embedding_table<span class="token punctuation">)</span>
  <span class="token keyword">else</span><span class="token punctuation">:</span>
    output <span class="token operator">=</span> tf<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>embedding_table<span class="token punctuation">,</span> flat_input_ids<span class="token punctuation">)</span>

  input_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>

  output <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>output<span class="token punctuation">,</span>
                      input_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>input_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> embedding_size<span class="token punctuation">]</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> <span class="token punctuation">(</span>output<span class="token punctuation">,</span> embedding_table<span class="token punctuation">)</span>
</code></pre>
<p>构造embedding_table，进行word embedding，可选one_hot的方式，返回embedding的结果和embedding_table<br>
<br></p>
<h4><a id="embedding_postprocessor_233"></a>embedding_postprocessor</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">embedding_postprocessor</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span>
                            use_token_type<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                            token_type_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                            token_type_vocab_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
                            token_type_embedding_name<span class="token operator">=</span><span class="token string">"token_type_embeddings"</span><span class="token punctuation">,</span>
                            use_position_embeddings<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                            position_embedding_name<span class="token operator">=</span><span class="token string">"position_embeddings"</span><span class="token punctuation">,</span>
                            initializer_range<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">,</span>
                            max_position_embeddings<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
                            dropout_prob<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Performs various post-processing on a word embedding tensor.

  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length,
      embedding_size].
    use_token_type: bool. Whether to add embeddings for `token_type_ids`.
    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
      Must be specified if `use_token_type` is True.
    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.
    token_type_embedding_name: string. The name of the embedding table variable
      for token type ids.
    use_position_embeddings: bool. Whether to add position embeddings for the
      position of each token in the sequence.
    position_embedding_name: string. The name of the embedding table variable
      for positional embeddings.
    initializer_range: float. Range of the weight initialization.
    max_position_embeddings: int. Maximum sequence length that might ever be
      used with this model. This can be longer than the sequence length of
      input_tensor, but cannot be shorter.
    dropout_prob: float. Dropout probability applied to the final output tensor.

  Returns:
    float tensor with same shape as `input_tensor`.

  Raises:
    ValueError: One of the tensor shapes or input values is invalid.
  """</span>
  input_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> expected_rank<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
  batch_size <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
  seq_length <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
  width <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>

  output <span class="token operator">=</span> input_tensor

  <span class="token keyword">if</span> use_token_type<span class="token punctuation">:</span>
    <span class="token keyword">if</span> token_type_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
      <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"`token_type_ids` must be specified if"</span>
                       <span class="token string">"`use_token_type` is True."</span><span class="token punctuation">)</span>
    token_type_table <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span>
        name<span class="token operator">=</span>token_type_embedding_name<span class="token punctuation">,</span>
        shape<span class="token operator">=</span><span class="token punctuation">[</span>token_type_vocab_size<span class="token punctuation">,</span> width<span class="token punctuation">]</span><span class="token punctuation">,</span>
        initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># This vocab will be small so we always do one-hot here, since it is always</span>
    <span class="token comment"># faster for a small vocabulary.</span>
    flat_token_type_ids <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>token_type_ids<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    one_hot_ids <span class="token operator">=</span> tf<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>flat_token_type_ids<span class="token punctuation">,</span> depth<span class="token operator">=</span>token_type_vocab_size<span class="token punctuation">)</span>
    token_type_embeddings <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>one_hot_ids<span class="token punctuation">,</span> token_type_table<span class="token punctuation">)</span>
    token_type_embeddings <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>token_type_embeddings<span class="token punctuation">,</span>
                                       <span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> width<span class="token punctuation">]</span><span class="token punctuation">)</span>
    output <span class="token operator">+=</span> token_type_embeddings

  <span class="token keyword">if</span> use_position_embeddings<span class="token punctuation">:</span>
    assert_op <span class="token operator">=</span> tf<span class="token punctuation">.</span>assert_less_equal<span class="token punctuation">(</span>seq_length<span class="token punctuation">,</span> max_position_embeddings<span class="token punctuation">)</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>control_dependencies<span class="token punctuation">(</span><span class="token punctuation">[</span>assert_op<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      full_position_embeddings <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span>
          name<span class="token operator">=</span>position_embedding_name<span class="token punctuation">,</span>
          shape<span class="token operator">=</span><span class="token punctuation">[</span>max_position_embeddings<span class="token punctuation">,</span> width<span class="token punctuation">]</span><span class="token punctuation">,</span>
          initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># position embedding table是一个learned variable，对[0, 1, 2, ..., max_position_embeddings-1]来讲，</span>
    <span class="token comment"># full_position_embeddings的shape就是[max_position_embeddings, width]</span>
    <span class="token comment">#</span>
    <span class="token comment"># 而当前的序列长度是seq_length，所以针对[0, 1, 2, ... seq_length-1], 可以对full_position_embeddings做个slice</span>
    <span class="token comment"># 传入给slice的begin是[0,0]，size是[seq_length,-1]，所以是对输入的shape取[0:seq_len, 0:-1]，所以</span>
    <span class="token comment"># slice的结果position_embeddings的shape是[seq_length, width]</span>
      position_embeddings <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token builtin">slice</span><span class="token punctuation">(</span>full_position_embeddings<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                     <span class="token punctuation">[</span>seq_length<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
      num_dims <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>shape<span class="token punctuation">.</span>as_list<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

      <span class="token comment"># Only the last two dimensions are relevant (`seq_length` and `width`), so</span>
      <span class="token comment"># we broadcast among the first dimensions, which is typically just</span>
      <span class="token comment"># the batch size.</span>
      position_broadcast_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
      <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_dims <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        position_broadcast_shape<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
      position_broadcast_shape<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>seq_length<span class="token punctuation">,</span> width<span class="token punctuation">]</span><span class="token punctuation">)</span>
      position_embeddings <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>position_embeddings<span class="token punctuation">,</span>
                                       position_broadcast_shape<span class="token punctuation">)</span>
      output <span class="token operator">+=</span> position_embeddings

  output <span class="token operator">=</span> layer_norm_and_dropout<span class="token punctuation">(</span>output<span class="token punctuation">,</span> dropout_prob<span class="token punctuation">)</span>
  <span class="token keyword">return</span> output
</code></pre>
<p>对词向量的后续处理，主要是添加位置信息和分段信息，token_type_embedding和position_embedding就对应着下图的segment embedding (训练句子匹配任务) 和 position embedding。<br>
<img src="https://img-blog.csdnimg.cn/2019070414321017.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<br></p>
<h4><a id="create_attention_mask_from_input_mask_331"></a>create_attention_mask_from_input_mask</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">create_attention_mask_from_input_mask</span><span class="token punctuation">(</span>from_tensor<span class="token punctuation">,</span> to_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""把2D的to_mask转为3D的。

  Args:
    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].  其实就是input_ids
    to_mask: int32 Tensor of shape [batch_size, to_seq_length].
    数据都是1

  Returns:
    float Tensor of shape [batch_size, from_seq_length, to_seq_length].
  """</span>
  from_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>from_tensor<span class="token punctuation">,</span> expected_rank<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
  batch_size <span class="token operator">=</span> from_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
  from_seq_length <span class="token operator">=</span> from_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

  to_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>to_mask<span class="token punctuation">,</span> expected_rank<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
  to_seq_length <span class="token operator">=</span> to_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

  to_mask <span class="token operator">=</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>
      tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>to_mask<span class="token punctuation">,</span> <span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> to_seq_length<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

  <span class="token comment"># We don't assume that `from_tensor` is a mask (although it could be). We</span>
  <span class="token comment"># don't actually care if we attend *from* padding tokens (only *to* padding)</span>
  <span class="token comment"># tokens so we create a tensor of all ones.</span>
  <span class="token comment">#</span>
  <span class="token comment"># `broadcast_ones` = [batch_size, from_seq_length, 1]</span>
  broadcast_ones <span class="token operator">=</span> tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>
      shape<span class="token operator">=</span><span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span> from_seq_length<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

  <span class="token comment"># Here we broadcast along two dimensions to create the mask.</span>
  mask <span class="token operator">=</span> broadcast_ones <span class="token operator">*</span> to_mask

  <span class="token keyword">return</span> mask
</code></pre>
<p>目前只看单文件，好像返回的是这个shape，但是数值都是1。<br>
<br></p>
<h4><a id="attention_layer_370"></a>attention_layer</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">attention_layer</span><span class="token punctuation">(</span>from_tensor<span class="token punctuation">,</span>
                    to_tensor<span class="token punctuation">,</span>
                    attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    num_attention_heads<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                    size_per_head<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
                    query_act<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    key_act<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    value_act<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    attention_probs_dropout_prob<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>
                    initializer_range<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">,</span>
                    do_return_2d_tensor<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                    batch_size<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    from_seq_length<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    to_seq_length<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Performs multi-headed attention from `from_tensor` to `to_tensor`.

  This is an implementation of multi-headed attention based on "Attention
  is all you Need". If `from_tensor` and `to_tensor` are the same, then
  this is self-attention. Each timestep in `from_tensor` attends to the
  corresponding sequence in `to_tensor`, and returns a fixed-with vector.

  This function first projects `from_tensor` into a "query" tensor and
  `to_tensor` into "key" and "value" tensors. These are (effectively) a list
  of tensors of length `num_attention_heads`, where each tensor is of shape
  [batch_size, seq_length, size_per_head].

  Then, the query and key tensors are dot-producted and scaled. These are
  softmaxed to obtain attention probabilities. The value tensors are then
  interpolated by these probabilities, then concatenated back to a single
  tensor and returned.

  In practice, the multi-headed attention are done with transposes and
  reshapes rather than actual separate tensors.

  Args:
    from_tensor: float Tensor of shape [batch_size, from_seq_length,
      from_width].
    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].
    attention_mask: (optional) int32 Tensor of shape [batch_size,
      from_seq_length, to_seq_length]. The values should be 1 or 0. The
      attention scores will effectively be set to -infinity for any positions in
      the mask that are 0, and will be unchanged for positions that are 1.
    num_attention_heads: int. Number of attention heads.
    size_per_head: int. Size of each attention head.
    query_act: (optional) Activation function for the query transform.
    key_act: (optional) Activation function for the key transform.
    value_act: (optional) Activation function for the value transform.
    attention_probs_dropout_prob: (optional) float. Dropout probability of the
      attention probabilities.
    initializer_range: float. Range of the weight initializer.
    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size
      * from_seq_length, num_attention_heads * size_per_head]. If False, the
      output will be of shape [batch_size, from_seq_length, num_attention_heads
      * size_per_head].
    batch_size: (Optional) int. If the input is 2D, this might be the batch size
      of the 3D version of the `from_tensor` and `to_tensor`.
    from_seq_length: (Optional) If the input is 2D, this might be the seq length
      of the 3D version of the `from_tensor`.
    to_seq_length: (Optional) If the input is 2D, this might be the seq length
      of the 3D version of the `to_tensor`.

  Returns:
    float Tensor of shape [batch_size, from_seq_length,
      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is
      true, this will be of shape [batch_size * from_seq_length,
      num_attention_heads * size_per_head]).

  Raises:
    ValueError: Any of the arguments or tensor shapes are invalid.
  """</span>

  <span class="token keyword">def</span> <span class="token function">transpose_for_scores</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> num_attention_heads<span class="token punctuation">,</span>
                           seq_length<span class="token punctuation">,</span> width<span class="token punctuation">)</span><span class="token punctuation">:</span>
    output_tensor <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
        input_tensor<span class="token punctuation">,</span> <span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> num_attention_heads<span class="token punctuation">,</span> width<span class="token punctuation">]</span><span class="token punctuation">)</span>

    output_tensor <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>output_tensor<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 二维就是转置，这里是4维，原来的维度是[0,1,2,3]，这里面的数字表示的</span>
    <span class="token comment"># 是维度的index，即变为[0,2,1,3]		[B, N, F或T, H]</span>
    <span class="token keyword">return</span> output_tensor

  from_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>from_tensor<span class="token punctuation">,</span> expected_rank<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
  to_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>to_tensor<span class="token punctuation">,</span> expected_rank<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
  <span class="token comment"># shape为2：([from_seq_length * seq_length, from_width])，或3：([batch_size, seq_length, from_width])</span>
  <span class="token comment"># 输入是embedding_output</span>

  <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>from_shape<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>to_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
        <span class="token string">"The rank of `from_tensor` must match the rank of `to_tensor`."</span><span class="token punctuation">)</span>

  <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>from_shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">:</span>
    batch_size <span class="token operator">=</span> from_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    from_seq_length <span class="token operator">=</span> from_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    to_seq_length <span class="token operator">=</span> to_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
  <span class="token keyword">elif</span> <span class="token builtin">len</span><span class="token punctuation">(</span>from_shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>batch_size <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token operator">or</span> from_seq_length <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token operator">or</span> to_seq_length <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
          <span class="token string">"When passing in rank 2 tensors to attention_layer, the values "</span>
          <span class="token string">"for `batch_size`, `from_seq_length`, and `to_seq_length` "</span>
          <span class="token string">"must all be specified."</span><span class="token punctuation">)</span>

  <span class="token comment"># Scalar dimensions referenced here:</span>
  <span class="token comment">#   B = batch size (number of sequences)</span>
  <span class="token comment">#   F = `from_tensor` sequence length</span>
  <span class="token comment">#   T = `to_tensor` sequence length</span>
  <span class="token comment">#   N = `num_attention_heads`</span>
  <span class="token comment">#   H = `size_per_head`</span>

  from_tensor_2d <span class="token operator">=</span> reshape_to_matrix<span class="token punctuation">(</span>from_tensor<span class="token punctuation">)</span>
  to_tensor_2d <span class="token operator">=</span> reshape_to_matrix<span class="token punctuation">(</span>to_tensor<span class="token punctuation">)</span>
  <span class="token comment"># 转化为2D向量，第二个维度为Width，第一个维度为batch_size*from_seq_length</span>

  <span class="token comment"># `query_layer` = [B*F, N*H]</span>
  query_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
      from_tensor_2d<span class="token punctuation">,</span>
      num_attention_heads <span class="token operator">*</span> size_per_head<span class="token punctuation">,</span>
      activation<span class="token operator">=</span>query_act<span class="token punctuation">,</span>
      name<span class="token operator">=</span><span class="token string">"query"</span><span class="token punctuation">,</span>
      kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token comment"># 通过全连接计算Query，第二个维度为num_attention_heads * size_per_head</span>
      <span class="token comment"># B*F理解为一个batch中词的个数，N*H理解为embed维度和head个数。</span>
      <span class="token comment"># 后面通过transpose转为多头。size_per_head(就是embedding_size)</span>


  <span class="token comment"># `key_layer` = [B*T, N*H]</span>
  key_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
      to_tensor_2d<span class="token punctuation">,</span>
      num_attention_heads <span class="token operator">*</span> size_per_head<span class="token punctuation">,</span>
      activation<span class="token operator">=</span>key_act<span class="token punctuation">,</span>
      name<span class="token operator">=</span><span class="token string">"key"</span><span class="token punctuation">,</span>
      kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>

  <span class="token comment"># `value_layer` = [B*T, N*H]</span>
  value_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
      to_tensor_2d<span class="token punctuation">,</span>
      num_attention_heads <span class="token operator">*</span> size_per_head<span class="token punctuation">,</span>
      activation<span class="token operator">=</span>value_act<span class="token punctuation">,</span>
      name<span class="token operator">=</span><span class="token string">"value"</span><span class="token punctuation">,</span>
      kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>

  <span class="token comment"># `query_layer` = [B, N, F, H]</span>
  query_layer <span class="token operator">=</span> transpose_for_scores<span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span>
                                     num_attention_heads<span class="token punctuation">,</span> from_seq_length<span class="token punctuation">,</span>
                                     size_per_head<span class="token punctuation">)</span>

  <span class="token comment"># `key_layer` = [B, N, T, H]</span>
  key_layer <span class="token operator">=</span> transpose_for_scores<span class="token punctuation">(</span>key_layer<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> num_attention_heads<span class="token punctuation">,</span>
                                   to_seq_length<span class="token punctuation">,</span> size_per_head<span class="token punctuation">)</span>

  <span class="token comment"># Take the dot product between "query" and "key" to get the raw</span>
  <span class="token comment"># attention scores.</span>
  <span class="token comment"># `attention_scores` = [B, N, F, T]</span>
  attention_scores <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> transpose_b<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  attention_scores <span class="token operator">=</span> tf<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>attention_scores<span class="token punctuation">,</span>
                                 <span class="token number">1.0</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>size_per_head<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token comment"># 计算公式。</span>

  <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token comment"># `attention_mask` = [B, 1, F, T]</span>
    attention_mask <span class="token operator">=</span> tf<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>attention_mask<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 在index=1的位置添加一个维度</span>

    <span class="token comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span>
    <span class="token comment"># masked positions, this operation will create a tensor which is 0.0 for</span>
    <span class="token comment"># positions we want to attend and -10000.0 for masked positions.</span>
    adder <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>attention_mask<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token operator">-</span><span class="token number">10000.0</span>
    <span class="token comment"># 对mask部分添加一个很大的负数</span>

    <span class="token comment"># Since we are adding it to the raw scores before the softmax, this is</span>
    <span class="token comment"># effectively the same as removing these entirely.</span>
    attention_scores <span class="token operator">+=</span> adder

  <span class="token comment"># Normalize the attention scores to probabilities.</span>
  <span class="token comment"># `attention_probs` = [B, N, F, T]</span>
  attention_probs <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention_scores<span class="token punctuation">)</span>
  <span class="token comment"># 对负数位置进行softmax后接近于0，就达成了对位置覆盖的任务。</span>
  <span class="token comment"># 如果没有mask也一样，对影响较小的词降低关注度。</span>
  <span class="token comment"># 公式中的softmax。</span>

  <span class="token comment"># This is actually dropping out entire tokens to attend to, which might</span>
  <span class="token comment"># seem a bit unusual, but is taken from the original Transformer paper.</span>
  attention_probs <span class="token operator">=</span> dropout<span class="token punctuation">(</span>attention_probs<span class="token punctuation">,</span> attention_probs_dropout_prob<span class="token punctuation">)</span>
  

  <span class="token comment"># `value_layer` = [B, T, N, H]</span>
  value_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
      value_layer<span class="token punctuation">,</span>
      <span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span> to_seq_length<span class="token punctuation">,</span> num_attention_heads<span class="token punctuation">,</span> size_per_head<span class="token punctuation">]</span><span class="token punctuation">)</span>

  <span class="token comment"># `value_layer` = [B, N, T, H]</span>
  value_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>value_layer<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  <span class="token comment"># `context_layer` = [B, N, F, H]</span>
  context_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_probs<span class="token punctuation">,</span> value_layer<span class="token punctuation">)</span>
  <span class="token comment"># 公式的最后一步，经过softmax输出的参数和V相乘。</span>

  <span class="token comment"># `context_layer` = [B, F, N, H]</span>
  context_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>context_layer<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  <span class="token keyword">if</span> do_return_2d_tensor<span class="token punctuation">:</span>
    <span class="token comment"># `context_layer` = [B*F, N*H]</span>
    context_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
        context_layer<span class="token punctuation">,</span>
        <span class="token punctuation">[</span>batch_size <span class="token operator">*</span> from_seq_length<span class="token punctuation">,</span> num_attention_heads <span class="token operator">*</span> size_per_head<span class="token punctuation">]</span><span class="token punctuation">)</span>
  <span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token comment"># `context_layer` = [B, F, N*H]</span>
    context_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
        context_layer<span class="token punctuation">,</span>
        <span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span> from_seq_length<span class="token punctuation">,</span> num_attention_heads <span class="token operator">*</span> size_per_head<span class="token punctuation">]</span><span class="token punctuation">)</span>

  <span class="token keyword">return</span> context_layer
</code></pre>
<p>每层都有一组QKV矩阵。在from_tensor和to_tensor相同的时候，就是self-attention，from_tensor其实就是Query矩阵，to_tensor表示的是Key=Value矩阵。计算过程如下：<br>
<img src="https://img-blog.csdnimg.cn/2019070417275658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<br></p>
<h4><a id="transformer_model_588"></a>transformer_model</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">transformer_model</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span>
                      attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                      hidden_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span>
                      num_hidden_layers<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span>
                      num_attention_heads<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span>
                      intermediate_size<span class="token operator">=</span><span class="token number">3072</span><span class="token punctuation">,</span>
                      intermediate_act_fn<span class="token operator">=</span>gelu<span class="token punctuation">,</span>
                      hidden_dropout_prob<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
                      attention_probs_dropout_prob<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
                      initializer_range<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">,</span>
                      do_return_all_layers<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".

  This is almost an exact implementation of the original Transformer encoder.

  See the original paper:
  https://arxiv.org/abs/1706.03762

  Also see:
  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py

  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].
    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,
      seq_length], with 1 for positions that can be attended to and 0 in
      positions that should not be.
    hidden_size: int. Hidden size of the Transformer.
    num_hidden_layers: int. Number of layers (blocks) in the Transformer.
    num_attention_heads: int. Number of attention heads in the Transformer.
    intermediate_size: int. The size of the "intermediate" (a.k.a., feed
      forward) layer.
    intermediate_act_fn: function. The non-linear activation function to apply
      to the output of the intermediate/feed-forward layer.
    hidden_dropout_prob: float. Dropout probability for the hidden layers.
    attention_probs_dropout_prob: float. Dropout probability of the attention
      probabilities.
    initializer_range: float. Range of the initializer (stddev of truncated
      normal).
    do_return_all_layers: Whether to also return all layers or just the final
      layer.

  Returns:
    float Tensor of shape [batch_size, seq_length, hidden_size], the final
    hidden layer of the Transformer.

  Raises:
    ValueError: A Tensor shape or parameter is invalid.
  """</span>
  <span class="token keyword">if</span> hidden_size <span class="token operator">%</span> num_attention_heads <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
        <span class="token string">"保证hidden_size能够整除num_attention_head 即将隐层输出分给各个head"</span>
        <span class="token string">"heads (%d)"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> num_attention_heads<span class="token punctuation">)</span><span class="token punctuation">)</span>

  attention_head_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>hidden_size <span class="token operator">/</span> num_attention_heads<span class="token punctuation">)</span>
  input_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> expected_rank<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
  batch_size <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
  seq_length <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
  input_width <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>

  <span class="token comment"># The Transformer performs sum residuals on all layers so the input needs</span>
  <span class="token comment"># to be the same as the hidden size.</span>
  <span class="token keyword">if</span> input_width <span class="token operator">!=</span> hidden_size<span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"The width of the input tensor (%d) != hidden size (%d)"</span> <span class="token operator">%</span>
                     <span class="token punctuation">(</span>input_width<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>

  <span class="token comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span>
  <span class="token comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span>
  <span class="token comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span>
  <span class="token comment"># help the optimizer.</span>
  prev_output <span class="token operator">=</span> reshape_to_matrix<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>

  all_layer_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> layer_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token comment"># 循环num_hidden_layers个模块就组成了transformer结构。</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"layer_%d"</span> <span class="token operator">%</span> layer_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
      layer_input <span class="token operator">=</span> prev_output

      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"attention"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        attention_heads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"self"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          attention_head <span class="token operator">=</span> attention_layer<span class="token punctuation">(</span>
              from_tensor<span class="token operator">=</span>layer_input<span class="token punctuation">,</span>
              to_tensor<span class="token operator">=</span>layer_input<span class="token punctuation">,</span>
              attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
              num_attention_heads<span class="token operator">=</span>num_attention_heads<span class="token punctuation">,</span>
              size_per_head<span class="token operator">=</span>attention_head_size<span class="token punctuation">,</span>
              attention_probs_dropout_prob<span class="token operator">=</span>attention_probs_dropout_prob<span class="token punctuation">,</span>
              initializer_range<span class="token operator">=</span>initializer_range<span class="token punctuation">,</span>
              do_return_2d_tensor<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
              batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span>
              from_seq_length<span class="token operator">=</span>seq_length<span class="token punctuation">,</span>
              to_seq_length<span class="token operator">=</span>seq_length<span class="token punctuation">)</span>
          attention_heads<span class="token punctuation">.</span>append<span class="token punctuation">(</span>attention_head<span class="token punctuation">)</span>

        attention_output <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>attention_heads<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token comment"># head的个数，如果是一个的话直接就是输出，多个需要拼接够送入下个block</span>
          attention_output <span class="token operator">=</span> attention_heads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
          <span class="token comment"># In the case where we have other sequences, we just concatenate</span>
          <span class="token comment"># them to the self-attention head before the projection.</span>
          attention_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span>attention_heads<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># Run a linear projection of `hidden_size` then add a residual</span>
        <span class="token comment"># with `layer_input`.</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"output"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          attention_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
              attention_output<span class="token punctuation">,</span>
              hidden_size<span class="token punctuation">,</span>
              kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>
          attention_output <span class="token operator">=</span> dropout<span class="token punctuation">(</span>attention_output<span class="token punctuation">,</span> hidden_dropout_prob<span class="token punctuation">)</span>
          attention_output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>attention_output <span class="token operator">+</span> layer_input<span class="token punctuation">)</span>
          <span class="token comment"># MLP+dropout+layer_norm。维度没变。</span>

      <span class="token comment"># The activation is only applied to the "intermediate" hidden layer.</span>
      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"intermediate"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        intermediate_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
            attention_output<span class="token punctuation">,</span>
            intermediate_size<span class="token punctuation">,</span>
            activation<span class="token operator">=</span>intermediate_act_fn<span class="token punctuation">,</span>
            kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token comment"># 继续MLP+dropout+layer_norm。intermediate_size是超参数</span>
            <span class="token comment"># 可自定，此时维度由[B * F, hidden_size]到</span>
            <span class="token comment"># [B * F, intermediate_size]。</span>

      <span class="token comment"># Down-project back to `hidden_size` then add the residual.</span>
      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"output"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        layer_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
            intermediate_output<span class="token punctuation">,</span>
            hidden_size<span class="token punctuation">,</span>
            kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>
        layer_output <span class="token operator">=</span> dropout<span class="token punctuation">(</span>layer_output<span class="token punctuation">,</span> hidden_dropout_prob<span class="token punctuation">)</span>
        layer_output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>layer_output <span class="token operator">+</span> attention_output<span class="token punctuation">)</span>
        prev_output <span class="token operator">=</span> layer_output
        <span class="token comment"># 最终输出，MLP+dropout+layer_norm，残差。</span>
        <span class="token comment"># [B * F, intermediate_size]又到[B * F, hidden_size]。</span>
        all_layer_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer_output<span class="token punctuation">)</span>
        <span class="token comment"># 这是记录每层的输出。</span>

  <span class="token keyword">if</span> do_return_all_layers<span class="token punctuation">:</span>
  <span class="token comment"># 可选是否返回所有层的输出结果。</span>
    final_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer_output <span class="token keyword">in</span> all_layer_outputs<span class="token punctuation">:</span>
      final_output <span class="token operator">=</span> reshape_from_matrix<span class="token punctuation">(</span>layer_output<span class="token punctuation">,</span> input_shape<span class="token punctuation">)</span>
      final_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>final_output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> final_outputs
  <span class="token keyword">else</span><span class="token punctuation">:</span>
    final_output <span class="token operator">=</span> reshape_from_matrix<span class="token punctuation">(</span>prev_output<span class="token punctuation">,</span> input_shape<span class="token punctuation">)</span>
    <span class="token keyword">return</span> final_output
</code></pre>
<br>
<h4><a id="get_shape_list_742"></a>get_shape_list</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_shape_list</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> expected_rank<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Returns a list of the shape of tensor, preferring static dimensions.

  Args:
    tensor: A tf.Tensor object to find the shape of.
    expected_rank: (optional) int. The expected rank of `tensor`. If this is
      specified and the `tensor` has a different rank, and exception will be
      thrown.
    name: Optional name of the tensor for the error message.

  Returns:
    A list of dimensions of the shape of tensor. All static dimensions will
    be returned as python integers, and dynamic dimensions will be returned
    as tf.Tensor scalars.
  """</span>
  <span class="token keyword">if</span> name <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> tensor<span class="token punctuation">.</span>name

  <span class="token keyword">if</span> expected_rank <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    assert_rank<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> expected_rank<span class="token punctuation">,</span> name<span class="token punctuation">)</span>

  shape <span class="token operator">=</span> tensor<span class="token punctuation">.</span>shape<span class="token punctuation">.</span>as_list<span class="token punctuation">(</span><span class="token punctuation">)</span>

  non_static_indexes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>index<span class="token punctuation">,</span> dim<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> dim <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
      non_static_indexes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>index<span class="token punctuation">)</span>

  <span class="token keyword">if</span> <span class="token operator">not</span> non_static_indexes<span class="token punctuation">:</span>
    <span class="token keyword">return</span> shape

  dyn_shape <span class="token operator">=</span> tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
  <span class="token keyword">for</span> index <span class="token keyword">in</span> non_static_indexes<span class="token punctuation">:</span>
    shape<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> dyn_shape<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
  <span class="token keyword">return</span> shape
</code></pre>
<p>返回一个list格式的shape大小。（类似[3,4,5] 表示维度是3，4，5？）<br>
<br></p>
<h4><a id="reshape_tofrom_matrix_783"></a>reshape_to/from_matrix</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">reshape_to_matrix</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Reshapes a &gt;= rank 2 tensor to a rank 2 tensor (i.e., a matrix)."""</span>
  ndims <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">.</span>ndims
  <span class="token keyword">if</span> ndims <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Input tensor must have at least rank 2. Shape = %s"</span> <span class="token operator">%</span>
                     <span class="token punctuation">(</span>input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token keyword">if</span> ndims <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> input_tensor

  width <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
  output_tensor <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> width<span class="token punctuation">]</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> output_tensor


<span class="token keyword">def</span> <span class="token function">reshape_from_matrix</span><span class="token punctuation">(</span>output_tensor<span class="token punctuation">,</span> orig_shape_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Reshapes a rank 2 tensor back to its original rank &gt;= 2 tensor."""</span>
  <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>orig_shape_list<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> output_tensor

  output_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>output_tensor<span class="token punctuation">)</span>

  orig_dims <span class="token operator">=</span> orig_shape_list<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
  width <span class="token operator">=</span> output_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

  <span class="token keyword">return</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>output_tensor<span class="token punctuation">,</span> orig_dims <span class="token operator">+</span> <span class="token punctuation">[</span>width<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
<p>高维变二维，二维变高维。<br>
<br></p>
<h4><a id="assert_rank_814"></a>assert_rank</h4>
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">assert_rank</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> expected_rank<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Raises an exception if the tensor rank is not of the expected rank.

  Args:
    tensor: A tf.Tensor to check the rank of.
    expected_rank: Python integer or list of integers, expected rank.
    name: Optional name of the tensor for the error message.

  Raises:
    ValueError: If the expected shape doesn't match the actual shape.
  """</span>
  <span class="token keyword">if</span> name <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> tensor<span class="token punctuation">.</span>name

  expected_rank_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
  <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>expected_rank<span class="token punctuation">,</span> six<span class="token punctuation">.</span>integer_types<span class="token punctuation">)</span><span class="token punctuation">:</span>
    expected_rank_dict<span class="token punctuation">[</span>expected_rank<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">True</span>
  <span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> x <span class="token keyword">in</span> expected_rank<span class="token punctuation">:</span>
      expected_rank_dict<span class="token punctuation">[</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">True</span>

  actual_rank <span class="token operator">=</span> tensor<span class="token punctuation">.</span>shape<span class="token punctuation">.</span>ndims
  <span class="token keyword">if</span> actual_rank <span class="token operator">not</span> <span class="token keyword">in</span> expected_rank_dict<span class="token punctuation">:</span>
    scope_name <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable_scope<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>name
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
        <span class="token string">"For the tensor `%s` in scope `%s`, the actual rank "</span>
        <span class="token string">"`%d` (shape = %s) is not equal to the expected rank `%s`"</span> <span class="token operator">%</span>
        <span class="token punctuation">(</span>name<span class="token punctuation">,</span> scope_name<span class="token punctuation">,</span> actual_rank<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">(</span>expected_rank<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p>rank表示的是维度，不是秩。如果输入的tensor的rank和预期不一样就会抛出异常。<br>
<br><br>
<br></p>
<h2><a id="BertModel_849"></a>BertModel</h2>
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BertModel</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""BERT model ("Bidirectional Encoder Representations from Transformers").

  Example usage:

  ```python
  # Already been converted into WordPiece token ids
  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])
  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])
  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])

  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,
    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)

  model = modeling.BertModel(config=config, is_training=True,
    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)

  label_embeddings = tf.get_variable(...)
  pooled_output = model.get_pooled_output()
  logits = tf.matmul(pooled_output, label_embeddings)
  ...
  """</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
               config<span class="token punctuation">,</span>
               is_training<span class="token punctuation">,</span>
               input_ids<span class="token punctuation">,</span>
               input_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
               token_type_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
               use_one_hot_embeddings<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
               scope<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Constructor for BertModel.

    Args:
      config: `BertConfig` instance.
      is_training: bool. true for training model, false for eval model. Controls
        whether dropout will be applied.
      input_ids: int32 Tensor of shape [batch_size, seq_length].
      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].
      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word
        embeddings or tf.embedding_lookup() for the word embeddings.
      scope: (optional) variable scope. Defaults to "bert".

    Raises:
      ValueError: The config is invalid or one of the input tensor shapes
        is invalid.
    """</span>
    config <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token operator">not</span> is_training<span class="token punctuation">:</span>
      config<span class="token punctuation">.</span>hidden_dropout_prob <span class="token operator">=</span> <span class="token number">0.0</span>
      config<span class="token punctuation">.</span>attention_probs_dropout_prob <span class="token operator">=</span> <span class="token number">0.0</span>

    input_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> expected_rank<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
    batch_size <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    seq_length <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token keyword">if</span> input_mask <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
      input_mask <span class="token operator">=</span> tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>

    <span class="token keyword">if</span> token_type_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
      token_type_ids <span class="token operator">=</span> tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">[</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>

    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>scope<span class="token punctuation">,</span> default_name<span class="token operator">=</span><span class="token string">"bert"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"embeddings"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Perform embedding lookup on the word ids.</span>
        <span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding_output<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embedding_table<span class="token punctuation">)</span> <span class="token operator">=</span> embedding_lookup<span class="token punctuation">(</span>
            input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span>
            vocab_size<span class="token operator">=</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span>
            embedding_size<span class="token operator">=</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            initializer_range<span class="token operator">=</span>config<span class="token punctuation">.</span>initializer_range<span class="token punctuation">,</span>
            word_embedding_name<span class="token operator">=</span><span class="token string">"word_embeddings"</span><span class="token punctuation">,</span>
            use_one_hot_embeddings<span class="token operator">=</span>use_one_hot_embeddings<span class="token punctuation">)</span>

        <span class="token comment"># Add positional embeddings and token type embeddings, then layer</span>
        <span class="token comment"># normalize and perform dropout.</span>
        self<span class="token punctuation">.</span>embedding_output <span class="token operator">=</span> embedding_postprocessor<span class="token punctuation">(</span>
            input_tensor<span class="token operator">=</span>self<span class="token punctuation">.</span>embedding_output<span class="token punctuation">,</span>
            use_token_type<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
            token_type_ids<span class="token operator">=</span>token_type_ids<span class="token punctuation">,</span>
            token_type_vocab_size<span class="token operator">=</span>config<span class="token punctuation">.</span>type_vocab_size<span class="token punctuation">,</span>
            token_type_embedding_name<span class="token operator">=</span><span class="token string">"token_type_embeddings"</span><span class="token punctuation">,</span>
            use_position_embeddings<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
            position_embedding_name<span class="token operator">=</span><span class="token string">"position_embeddings"</span><span class="token punctuation">,</span>
            initializer_range<span class="token operator">=</span>config<span class="token punctuation">.</span>initializer_range<span class="token punctuation">,</span>
            max_position_embeddings<span class="token operator">=</span>config<span class="token punctuation">.</span>max_position_embeddings<span class="token punctuation">,</span>
            dropout_prob<span class="token operator">=</span>config<span class="token punctuation">.</span>hidden_dropout_prob<span class="token punctuation">)</span>

      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"encoder"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span>
        <span class="token comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span>
        <span class="token comment"># for the attention scores.</span>
        attention_mask <span class="token operator">=</span> create_attention_mask_from_input_mask<span class="token punctuation">(</span>
            input_ids<span class="token punctuation">,</span> input_mask<span class="token punctuation">)</span>

        <span class="token comment"># Run the stacked transformer.</span>
        <span class="token comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span>
        self<span class="token punctuation">.</span>all_encoder_layers <span class="token operator">=</span> transformer_model<span class="token punctuation">(</span>
            input_tensor<span class="token operator">=</span>self<span class="token punctuation">.</span>embedding_output<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            hidden_size<span class="token operator">=</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            num_hidden_layers<span class="token operator">=</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">,</span>
            num_attention_heads<span class="token operator">=</span>config<span class="token punctuation">.</span>num_attention_heads<span class="token punctuation">,</span>
            intermediate_size<span class="token operator">=</span>config<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span>
            intermediate_act_fn<span class="token operator">=</span>get_activation<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_act<span class="token punctuation">)</span><span class="token punctuation">,</span>
            hidden_dropout_prob<span class="token operator">=</span>config<span class="token punctuation">.</span>hidden_dropout_prob<span class="token punctuation">,</span>
            attention_probs_dropout_prob<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_probs_dropout_prob<span class="token punctuation">,</span>
            initializer_range<span class="token operator">=</span>config<span class="token punctuation">.</span>initializer_range<span class="token punctuation">,</span>
            do_return_all_layers<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

      self<span class="token punctuation">.</span>sequence_output <span class="token operator">=</span> self<span class="token punctuation">.</span>all_encoder_layers<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
      <span class="token comment"># The "pooler" converts the encoded sequence tensor of shape</span>
      <span class="token comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span>
      <span class="token comment"># [batch_size, hidden_size]. This is necessary for segment-level</span>
      <span class="token comment"># (or segment-pair-level) classification tasks where we need a fixed</span>
      <span class="token comment"># dimensional representation of the segment.</span>
      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"pooler"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># We "pool" the model by simply taking the hidden state corresponding</span>
        <span class="token comment"># to the first token. We assume that this has been pre-trained</span>
        first_token_tensor <span class="token operator">=</span> tf<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sequence_output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pooled_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
            first_token_tensor<span class="token punctuation">,</span>
            config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
            activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>tanh<span class="token punctuation">,</span>
            kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>config<span class="token punctuation">.</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">get_pooled_output</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>pooled_output

  <span class="token keyword">def</span> <span class="token function">get_sequence_output</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Gets final hidden layer of encoder.

    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the final hidden of the transformer encoder.
    """</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>sequence_output

  <span class="token keyword">def</span> <span class="token function">get_all_encoder_layers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>all_encoder_layers

  <span class="token keyword">def</span> <span class="token function">get_embedding_output</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Gets output of the embedding lookup (i.e., input to the transformer).

    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the output of the embedding layer, after summing the word
      embeddings with the positional embeddings and the token type embeddings,
      then performing layer normalization. This is the input to the transformer.
    """</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>embedding_output

  <span class="token keyword">def</span> <span class="token function">get_embedding_table</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>embedding_table
</code></pre>
<p>模型入口：<br>
(1）设置各种参数，如果input_mask为None的话，就指定所有input_mask值为1，即不进行过滤；如果token_type_ids是None的话，就指定所有token_type_ids值为0；<br>
(2）对输入的input_ids进行embedding操作，再embedding_postprocessor操作，前面我们说了。主要是加入位置和token_type信息到词向量里面；<br>
(3）转换attention_mask 后，通过调用transformer_model进行encoder操作；<br>
(4）获取最后一层的输出sequence_output和pooled_output，pooled_output是取sequence_output的第一个切片然后线性投影获得（可以用于分类问题）</p>
<h2><a id="BertConfig_1012"></a>BertConfig</h2>
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BertConfig</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Configuration for `BertModel`."""</span>

  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
               vocab_size<span class="token punctuation">,</span>
               hidden_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span>
               num_hidden_layers<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span>
               num_attention_heads<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span>
               intermediate_size<span class="token operator">=</span><span class="token number">3072</span><span class="token punctuation">,</span>
               hidden_act<span class="token operator">=</span><span class="token string">"gelu"</span><span class="token punctuation">,</span>
               hidden_dropout_prob<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
               attention_probs_dropout_prob<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
               max_position_embeddings<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
               type_vocab_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
               initializer_range<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Constructs BertConfig.

    Args:
      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.
      hidden_size: Size of the encoder layers and the pooler layer.
      num_hidden_layers: Number of hidden layers in the Transformer encoder.
      num_attention_heads: Number of attention heads for each attention layer in
        the Transformer encoder.
      intermediate_size: The size of the "intermediate" (i.e., feed-forward)
        layer in the Transformer encoder.
      hidden_act: The non-linear activation function (function or string) in the
        encoder and pooler.
      hidden_dropout_prob: The dropout probability for all fully connected
        layers in the embeddings, encoder, and pooler.
      attention_probs_dropout_prob: The dropout ratio for the attention
        probabilities.
      max_position_embeddings: The maximum sequence length that this model might
        ever be used with. Typically set this to something large just in case
        (e.g., 512 or 1024 or 2048).
      type_vocab_size: The vocabulary size of the `token_type_ids` passed into
        `BertModel`.
      initializer_range: The stdev of the truncated_normal_initializer for
        initializing all weight matrices.
    """</span>
    self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> vocab_size
    self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
    self<span class="token punctuation">.</span>num_hidden_layers <span class="token operator">=</span> num_hidden_layers
    self<span class="token punctuation">.</span>num_attention_heads <span class="token operator">=</span> num_attention_heads
    self<span class="token punctuation">.</span>hidden_act <span class="token operator">=</span> hidden_act
    self<span class="token punctuation">.</span>intermediate_size <span class="token operator">=</span> intermediate_size
    self<span class="token punctuation">.</span>hidden_dropout_prob <span class="token operator">=</span> hidden_dropout_prob
    self<span class="token punctuation">.</span>attention_probs_dropout_prob <span class="token operator">=</span> attention_probs_dropout_prob
    self<span class="token punctuation">.</span>max_position_embeddings <span class="token operator">=</span> max_position_embeddings
    self<span class="token punctuation">.</span>type_vocab_size <span class="token operator">=</span> type_vocab_size
    self<span class="token punctuation">.</span>initializer_range <span class="token operator">=</span> initializer_range

  @<span class="token builtin">classmethod</span>
  <span class="token keyword">def</span> <span class="token function">from_dict</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> json_object<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Constructs a `BertConfig` from a Python dictionary of parameters."""</span>
    config <span class="token operator">=</span> BertConfig<span class="token punctuation">(</span>vocab_size<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>key<span class="token punctuation">,</span> value<span class="token punctuation">)</span> <span class="token keyword">in</span> six<span class="token punctuation">.</span>iteritems<span class="token punctuation">(</span>json_object<span class="token punctuation">)</span><span class="token punctuation">:</span>
      config<span class="token punctuation">.</span>__dict__<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> value
    <span class="token keyword">return</span> config

  @<span class="token builtin">classmethod</span>
  <span class="token keyword">def</span> <span class="token function">from_json_file</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> json_file<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Constructs a `BertConfig` from a json file of parameters."""</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>gfile<span class="token punctuation">.</span>GFile<span class="token punctuation">(</span>json_file<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> reader<span class="token punctuation">:</span>
      text <span class="token operator">=</span> reader<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> cls<span class="token punctuation">.</span>from_dict<span class="token punctuation">(</span>json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">to_dict</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Serializes this instance to a Python dictionary."""</span>
    output <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>self<span class="token punctuation">.</span>__dict__<span class="token punctuation">)</span>
    <span class="token keyword">return</span> output

  <span class="token keyword">def</span> <span class="token function">to_json_string</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Serializes this instance to a JSON string."""</span>
    <span class="token keyword">return</span> json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span>self<span class="token punctuation">.</span>to_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> indent<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> sort_keys<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n"</span>
</code></pre>
<p>模型配置，依次是：词典大小、隐层神经元个数、transformer的层数、attention的头数、激活函数、中间层神经元个数、隐层dropout比例、attention里面dropout比例、sequence最大长度、token_type_ids的词典大小、truncated_normal_initializer的stdev。</p>
</div>
</body>

</html>
