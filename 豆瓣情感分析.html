<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>实习--豆瓣分析</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1><a id="_0"></a>学习笔记</h1>
<blockquote>
<p>可能有部分内容摘自他人文章，如果侵权请告知，会立即删除。</p>
</blockquote>
<h2><a id="NLP_2"></a>NLP</h2>
<p><strong>豆瓣影评情感分析</strong> ：今天第一个任务是将kaggle下载的豆瓣影评数据做预处理，然后导入到昨天弄好的酒店评论情感分析的模型中。</p>
<ul>
<li>预处理部分先读入csv文件，然后保留星级评价和评论文字，接着转换星级为需求的属性作为数据的label。</li>
</ul>
<pre><code class="prism language-python">data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'DMSC.csv'</span><span class="token punctuation">)</span>
label_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'Star'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>values<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 此处先做简单星级处理，123分为负面，45为正面评论</span>
corpus_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'Comment'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># array读取后转为list，方便后面做操作</span>
</code></pre>
<p>计算一下正负样例的比例:</p>
<pre><code class="prism language-python"><span class="token comment"># 计算正负样本比重</span>
np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>label_data<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>label_data<span class="token punctuation">)</span>
</code></pre>
<p>结果是0.6左右，还可以，凑活先用吧，后面再换大点的数据集。</p>
<ul>
<li>接下来对评论数据分词操作。先用re正则化把标点符号全部去掉，这是为了后面index化的操作，而且标点符号本身没有意义，会影响模型效果。两行代码都可以使用，个人认为第二行更好，因为观察到数据集中包含有表情符号之类的，第二行是只保留文字字母和数字，第一行是去除所有符号。</li>
</ul>
<pre><code class="prism language-python"><span class="token comment"># text = re.sub('[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&amp;*（）]+', '', text[0])</span>
text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>r<span class="token string">'[^\u4e00-\u9fa5a-zA-Z0-9]'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> text<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
<ul>
<li>然后是分词和Index化文字。分词使用jieba库的精简模式操作，后面应该会尝试使用其他拓展库的分词方法，github上有统计拓展的对专有名词的语料效果更好。</li>
</ul>
<pre><code class="prism language-python"><span class="token comment"># 进行分词和tokenize</span>
<span class="token comment"># train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价train_tokens = []</span>
train_target <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
target_flag <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># flag控制对应的label是否保留</span>
<span class="token keyword">for</span> text <span class="token keyword">in</span> corpus_data<span class="token punctuation">:</span>
	<span class="token comment"># 去掉标点</span>
	<span class="token comment"># text = re.sub('[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&amp;*（）]+', '', text[0])</span>
	text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>r<span class="token string">'[^\u4e00-\u9fa50-9]'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> text<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 这块调整了，删除了a-zA-Z,因为测试的时候发现，英文单词在现在使用的预训练好的词向量库中没有</span>
    <span class="token comment"># 对应的词向量，都是0，暂时没想到有什么比较好的替代办法，决定先删掉，后面应该会调整词库，想办</span>
    <span class="token comment"># 法得到英文单词的表达。</span>
    <span class="token comment"># 符号替换成空''</span>
    <span class="token comment"># 结巴分词</span>
    cut <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    <span class="token comment"># 结巴分词的输出结果为一个生成器   精简模式</span>
    <span class="token comment"># 把生成器转换为list</span>
    cut_list <span class="token operator">=</span> <span class="token punctuation">[</span> i <span class="token keyword">for</span> i <span class="token keyword">in</span> cut <span class="token punctuation">]</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> word <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>cut_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">try</span><span class="token punctuation">:</span>
            <span class="token comment"># 将词转换为索引index</span>
            cut_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> cn_model<span class="token punctuation">.</span>vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span><span class="token punctuation">.</span>index
            <span class="token comment"># 将句子中的词转为预训练好的词库中的对应词的index</span>
            <span class="token comment"># 这样就将句子分为词的list转为存的都是词对应的index的list</span>
        <span class="token keyword">except</span> KeyError<span class="token punctuation">:</span>
            <span class="token comment"># 如果词不在字典中，则输出0</span>
            cut_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">if</span> <span class="token operator">not</span> cut_list <span class="token operator">==</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        train_tokens<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cut_list<span class="token punctuation">)</span>
        train_target<span class="token punctuation">.</span>append<span class="token punctuation">(</span>label_data<span class="token punctuation">[</span>target_flag<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># 删除掉长度为0的语料和对应的label</span>
    target_flag <span class="token operator">+=</span> <span class="token number">1</span>
    <span class="token comment"># flag自增，目的是删除对应的label</span>
</code></pre>
<p>最后得到所有语料的index，格式为二维list。删除了长度为0的语料。(因为观察到有些评论全都是符号，或者全是英文评论的。词库没有对应的英文vector，分词后就为空。)</p>
<ul>
<li>得到语料的分词index后，先要对数据进行采样和清洗，去除边缘比较特殊的数据，根据分布情况，类似高斯。</li>
</ul>
<pre><code class="prism language-python">num_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>token<span class="token punctuation">)</span> <span class="token keyword">for</span> token <span class="token keyword">in</span> train_tokens<span class="token punctuation">]</span>
num_tokens <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>num_tokens<span class="token punctuation">)</span>
<span class="token comment"># 计算每个语料分词后的长度</span>

plt<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>num_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> bins <span class="token operator">=</span> <span class="token number">12</span><span class="token punctuation">)</span>
<span class="token comment"># bins是柱状图的柱数目。  log对每个句子的长度取，底数为2</span>
<span class="token comment"># 取了log之后发现基本是正太分布。</span>
plt<span class="token punctuation">.</span>xlim<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'number of tokens'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'length of tokens'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Distribution of tokens length'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 画出语料长度的分布情况</span>

max_tokens <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>num_tokens<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>num_tokens<span class="token punctuation">)</span>
max_tokens <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>max_tokens<span class="token punctuation">)</span>
<span class="token comment"># np.sum( num_tokens &lt; max_tokens ) / len(num_tokens)</span>
<span class="token comment"># 计算max_tokens，截取2sigma长度后max_tokens=52,数据保留率为92.6%。</span>
<span class="token comment"># 3sigma长度max_tokens=70,数据保留率为97.4%。</span>
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/201906211411394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt=""></p>
<ul>
<li>然后就是建立Embedding层了，还建立了一个从index转为文字的函数。</li>
</ul>
<pre><code class="prism language-python"><span class="token comment"># 用来将tokens转换为文本</span>
<span class="token keyword">def</span> <span class="token function">reverse_tokens</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
    text <span class="token operator">=</span> <span class="token string">''</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> tokens<span class="token punctuation">:</span>
        <span class="token keyword">if</span> i <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            text <span class="token operator">=</span> text <span class="token operator">+</span> cn_model<span class="token punctuation">.</span>index2word<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            text <span class="token operator">=</span> text <span class="token operator">+</span> <span class="token string">' '</span>
    <span class="token keyword">return</span> text

reverse <span class="token operator">=</span> reverse_tokens<span class="token punctuation">(</span>train_tokens<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 查看从index2word的语料</span>

<span class="token comment"># 词库一共259883个词，通过get_</span>
num_words <span class="token operator">=</span> <span class="token number">259883</span>
<span class="token comment"># 初始化embedding_matrix，之后在keras上进行应用</span>
embedding_matrix <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>num_words<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># embedding_matrix为一个 [num_words，embedding_dim] 的矩阵</span>
<span class="token comment"># 维度为 50000 * 300</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_words<span class="token punctuation">)</span><span class="token punctuation">:</span>
    embedding_matrix<span class="token punctuation">[</span>i<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> cn_model<span class="token punctuation">[</span>cn_model<span class="token punctuation">.</span>index2word<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span>
embedding_matrix <span class="token operator">=</span> embedding_matrix<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span>
<span class="token comment"># 从预训练的词向量库中调出前50000个词的词向量，每个维度为300，即50000*300</span>
<span class="token comment"># 所以假设句子有n个词，那么句子的输入维度为n*50000，只有n个位置是1，其他都是0，稀疏矩阵</span>
<span class="token comment"># n*50000 和 embedding矩阵相乘得到 n*300的稠密矩阵，表示了n个词的句子。</span>

<span class="token comment"># 检查index是否对应，</span>
<span class="token comment"># 输出300意义为长度为300的embedding向量一一对应</span>
<span class="token comment"># np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )</span>
</code></pre>
<ul>
<li>Padding和truncating。将分词调整为相同大小。以进入NN中训练。</li>
</ul>
<pre><code class="prism language-python"><span class="token comment"># 输入的train_tokens是一个list</span>
<span class="token comment"># 返回的train_pad是一个numpy array</span>
train_pad <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>train_tokens<span class="token punctuation">,</span> maxlen<span class="token operator">=</span>max_tokens<span class="token punctuation">,</span>
                            padding<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">,</span> truncating<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">)</span>
<span class="token comment"># pad_sequences固定序列长度  maxlen:序列保留长度  padding:pre或post补零， truncating:pre或post截断</span>

<span class="token comment"># 将target从list格式转为array格式</span>
train_target <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>train_target<span class="token punctuation">)</span>
</code></pre>
<ul>
<li>建模。第一层Embedding层，第二层双向LSTM，第三层LSTM，最后全连接sigmoid进行二分类。Embedding层的输入为(batchsize, maxtokens)，输出为 (batchsize, maxtokens, embedding dim) 。batchsize是语料库大小：50851，maxtokens：52，embedding dim：300。双向LSTM的隐层节点数为64，因为双向，所以输出的维度为(None,52,128)。单向LSTM的节点数为16，所以输出维度为(None,16)。</li>
</ul>
<pre><code class="prism language-python"><span class="token comment"># 进行训练和测试样本的分割</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split

<span class="token comment"># 90%的样本用来训练，剩余10%用来测试</span>
X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>train_pad<span class="token punctuation">,</span>
                                                    train_target<span class="token punctuation">,</span>
                                                    test_size<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
                                                    random_state<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span>

<span class="token comment"># 查看训练样本，确认无误</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>reverse_tokens<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span><span class="token number">35</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'class: '</span><span class="token punctuation">,</span>y_train<span class="token punctuation">[</span><span class="token number">35</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 用LSTM对样本进行分类</span>
model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 模型第一层为embedding</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Embedding<span class="token punctuation">(</span>num_words<span class="token punctuation">,</span>
                    embedding_dim<span class="token punctuation">,</span>
                    weights<span class="token operator">=</span><span class="token punctuation">[</span>embedding_matrix<span class="token punctuation">]</span><span class="token punctuation">,</span>
                    input_length<span class="token operator">=</span>max_tokens<span class="token punctuation">,</span>
                    trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># num_words:259883, embedding_dim:300, max_tokens:52</span>
<span class="token comment"># embedding_matrix就是之前得到的259883个词的词向量</span>
<span class="token comment"># Embedding()就是输入embedding层的维度和矩阵，以及传入的语料的长度</span>
<span class="token comment"># Embedding层的输入为(batchsize, maxtokens)，输出为(batchsize, maxtokens, embedding dim)</span>
<span class="token comment"># batchsize是语料库大小：50851      maxtokens：52        embedding dim：300</span>

model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span>units<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 双向LSTM的隐层节点数为64，因为双向，所以输出的维度为(None,52,128)</span>
<span class="token comment"># 单向LSTM的节点数为16，所以输出维度为(None,16)</span>
<span class="token comment"># return_sequences: 输出全部时间步的hidden层输出结果</span>

model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 我们使用adam以0.001的learning rate进行优化</span>
optimizer <span class="token operator">=</span> Adam<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'binary_crossentropy'</span><span class="token punctuation">,</span>
              optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
<ul>
<li>模型保存，加载，训练，测试。定义了一个函数方便测试自己输入的文本的情况，而且可以看到最后的sigmoid输出的决策分数。目标是想要输出的分数更接近0或1，以表明模型有更大的把握确定语料的label归属。接近0.5则表明对判断结果不够确定，比较模糊。</li>
<li>callback是回调函数，包含了    earlystopping, checkpoint, lr_reduction三个内容。earlystopping是模型训练时如果超过3个epoch内valdation loss没有改善就停止训练。checkpoint是在训练过程中，可以保存参数的数值，当val_loss有改善时，储存权重。lr_reduction是自动降低学习速率。</li>
</ul>
<pre><code class="prism language-python"><span class="token comment"># 建立一个权重的存储点</span>
path_checkpoint <span class="token operator">=</span> <span class="token string">'sentiment_checkpoint.keras'</span>
checkpoint <span class="token operator">=</span> ModelCheckpoint<span class="token punctuation">(</span>filepath<span class="token operator">=</span>path_checkpoint<span class="token punctuation">,</span> monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span>
                                      verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> save_weights_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                      save_best_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 尝试加载已训练模型</span>
<span class="token keyword">try</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>path_checkpoint<span class="token punctuation">)</span>
<span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span>

<span class="token comment"># 定义early stoping如果3个epoch内validation loss没有改善则停止训练</span>
earlystopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 自动降低learning rate</span>
lr_reduction <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>monitor<span class="token operator">=</span><span class="token string">'val_loss'</span><span class="token punctuation">,</span>
                                       factor<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">8</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                                       verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 定义callback函数</span>
callbacks <span class="token operator">=</span> <span class="token punctuation">[</span>
    earlystopping<span class="token punctuation">,</span>
    checkpoint<span class="token punctuation">,</span>
    lr_reduction
<span class="token punctuation">]</span>

<span class="token comment"># 开始训练</span>
model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span>
          validation_split<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
          epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>
          batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>
          callbacks<span class="token operator">=</span>callbacks<span class="token punctuation">)</span>

result <span class="token operator">=</span> model<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy:{0:.2%}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>result<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token comment"># 自定义输入文本测试</span>
<span class="token keyword">def</span> <span class="token function">predict_sentiment</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    <span class="token comment"># 去标点</span>
    <span class="token comment"># text = re.sub("[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&amp;*（）]+", "",text)</span>
    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>r<span class="token string">'[^\u4e00-\u9fa50-9]'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">,</span> text<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 分词</span>
    cut <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    cut_list <span class="token operator">=</span> <span class="token punctuation">[</span> i <span class="token keyword">for</span> i <span class="token keyword">in</span> cut <span class="token punctuation">]</span>
    <span class="token comment"># tokenize</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> word <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>cut_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">try</span><span class="token punctuation">:</span>
            cut_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> cn_model<span class="token punctuation">.</span>vocab<span class="token punctuation">[</span>word<span class="token punctuation">]</span><span class="token punctuation">.</span>index
        <span class="token keyword">except</span> KeyError<span class="token punctuation">:</span>
            cut_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token comment"># padding</span>
    tokens_pad <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>cut_list<span class="token punctuation">]</span><span class="token punctuation">,</span> maxlen<span class="token operator">=</span>max_tokens<span class="token punctuation">,</span>
                           padding<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">,</span> truncating<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">)</span>
    <span class="token comment"># 预测</span>
    result <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token operator">=</span>tokens_pad<span class="token punctuation">)</span>
    coef <span class="token operator">=</span> result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">if</span> coef <span class="token operator">&gt;=</span> <span class="token number">0.5</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'是一例正面评价'</span><span class="token punctuation">,</span><span class="token string">'output=%.2f'</span><span class="token operator">%</span>coef<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'是一例负面评价'</span><span class="token punctuation">,</span><span class="token string">'output=%.2f'</span><span class="token operator">%</span>coef<span class="token punctuation">)</span>

test_list <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">'十年后重看只想大哭，其实我们都明白这个结局。'</span><span class="token punctuation">,</span>
    <span class="token string">'“你敢保证你一辈子不得病？”纯粹、直接、有力！常常感叹：电影只能是电影。但每看到这样的佳作，又感慨：电影不只是电影！由衷的希望这部电影大卖！成为话题！成为榜样！成为国产电影最该有的可能。'</span><span class="token punctuation">,</span>
    <span class="token string">'炸裂，哭成狗，从观影体验上看，比达拉斯买家俱乐部好，之间隔了差不多五个《动物世界》，导演处女作就这完成度，只能说剧本实在太好。我爸爸也是药神的受益者之一，否则我应该房子也没了。感谢他们'</span><span class="token punctuation">,</span>
    <span class="token string">'电影能做到的好，这部电影都做到了。剩下的是这个时代不让它更好。在我们刚刚经历过的时代巨变洪流之中，有无数这样的小人物在时代洪流中艰难生存着，同时在竭力不丢失他们的灵魂。终于有这样一部电影，让我们能够看到时代，看到善意，看到希望。希望这部电影也能被这个时代善待。'</span><span class="token punctuation">,</span>
    <span class="token string">'感觉看了个假的绿皮书，为什么大家全都五星好评？Viggo叔很棒，Ali一般，剧本烂俗至极，有如种族歧视101，老套剧情一个接一个，黑人坐车后一定要被南方警察拦下，哦对了还要加个北方警察送温暖来对比衬托，真是恶心死了。最后结局当然要在白人消除种族歧视的良好自我感觉中结束。'</span><span class="token punctuation">,</span>
    <span class="token string">'难看……有虐待动物三观不正以及精神胜利法，逻辑混乱等众多尬点，特效五毛到震惊……令人失望。 外星人其实没什么恶意，也没伤害哪个就被虐待和杀害的桥段让我很不舒服……总的来说故事很混乱，看了挺后悔'</span>
<span class="token punctuation">]</span>

<span class="token keyword">for</span> text <span class="token keyword">in</span> test_list<span class="token punctuation">:</span>
    predict_sentiment<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
</code></pre>
<h2><a id="_256"></a>结果分析</h2>
<p><img src="https://img-blog.csdnimg.cn/20190621143040987.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
<img src="https://img-blog.csdnimg.cn/20190621143458193.png" alt="在这里插入图片描述"></p>
<ul>
<li>最终收敛效果一般吧，训练集准确率有85%左右，交叉验证集和测试集79%左右。单从结果来看，训练效果还是可以接受的，因为并没有用十分复杂的网络，训练数据用的也比较少，因为为了能快速的看到结果，没有用全部的200+w条数据训练，只从中选择了大概5.2w条数据，增加数据应该会使效果有些提升。</li>
<li>在我自己去豆瓣上找了几条影评来做测试时，效果感觉并不好。思考了一下，应该跟词向量有很大关系(废话)。后面会从几个方面考虑：
<ul>
<li>更大的词向量库</li>
<li>载入更多训练数据训练</li>
<li>修改模型，可能两层LSTM太简单了？会去搜一下比较新的模型试试看<br>
<img src="https://img-blog.csdnimg.cn/20190621144117524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
</li>
</ul>
</div>
</body>

</html>
