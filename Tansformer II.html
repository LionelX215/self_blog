<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Tansformer II</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><p>看了一个讲解视频，记录一下。</p>
<h4><a id="Selfattention_1"></a>Self-attention</h4>
<p>self-attention layer本质是用来替代CNN和RNN的，可以像CNN一样并行输入，输出可以同时得到，不需要像RNN一样，在得到第二个输出之前一定要有第一个的输出。并且又可以在浅层得到其他词的信息，与句中所有位置的词的距离都相等，都可以获得它们的信息。<br>
接下来看一下Self-attention的内部结构。<br>
首先每个tokens经过一个embedding，获得a1-a4，然后输入到self-attention layer，每一个input分别乘上三个不同的matrix，产生不同的vector，叫做Q,K,V。<br>
<img src="https://img-blog.csdnimg.cn/20190628143750125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>然后，拿每个query Q 对每个 key K 做 attention。其实attention就是计算两个向量之间的匹配程度（考虑为重要性？每个token对目标token的重要性？），<br>
<img src="https://img-blog.csdnimg.cn/20190628144154479.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
在self-attention中，attention的计算公式为：<br>
<img src="https://img-blog.csdnimg.cn/20190628144345311.png" alt="在这里插入图片描述"><br>
dot product 表示点积。注意除以了根号d，这是因为α会随着q和k的维度增大而增大，所以除以这个值以平衡使不会太大。得到α后，经过一个softmax层得到alpha_hat，	<br>
<img src="https://img-blog.csdnimg.cn/20190628145401614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
然后根据得到的alpha_hat，与v做计算，得到每一层的b，<br>
<img src="https://img-blog.csdnimg.cn/20190628145549727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
这里就已经能够看出，得到vector b 的过程中使用了整sequence的信息，因为b是对v的weight sum 得到的，而v又是从a进行transformation得到的，所以产生b的时候，已经看遍了a_1到a_4的内容。如果想要b1只考虑a1自己的信息，那么只需要让alpha_hat 2 3 4 为0就可以了，就相当于没有考虑他们的信息。所以对于self-attention来说，无论是近的还是远的信息，只要需要，就可以得到。在计算b1的同时，b2也可以计算，是并行的，计算过程一样，只不过用的q变为了q2：</p>
<p><img src="https://img-blog.csdnimg.cn/20190628151049995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="Multihead_Selfattention_18"></a>Multi-head Self-attention</h4>
<p>其实和普通的self-attention过程是一样的，只不过每个alpha的matrix w q v由一组变成了多组，<br>
<img src="https://img-blog.csdnimg.cn/20190628152646663.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
最终得到的多个b_i拼接到一起，如果这个dim不满足自己的需求，可以设定一个W_0对它做一个降维：<br>
<img src="https://img-blog.csdnimg.cn/20190628152756514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
Multi-head 的好处在于，有可能不同的head最终关注的点是不同的，可能有的head关注的是比较近的邻居的信息，有的head关注的是比较远的位置的邻居信息。</p>
<h4><a id="Positional_Encoding_25"></a>Positional Encoding</h4>
<p>经过上面，其实模型还有一个问题存在，就是次序的问题，每个输出b相当于是句中所有a的信息的融合，然而a的顺序是不能影响b的输出的，举例来说就是输入“我打你”和“你打我”，虽然是完全不同的意思，但在self-attention模型中得到的结果是完全相同的。<br>
我们希望模型能够读到位置信息，所以paper中添加了一个vector e_i，这个e_i的dim和a_i是相同的，在a_i和matrix做计算之前被加入到a_i中来，注意e_i不是学习得到的，而是提前设置的。e_i是加入到a_i中的，而不是拼接的，可以推导，利用one-hot去记录位置信息，将one-hot码拼接到token x_i中，<br>
<img src="https://img-blog.csdnimg.cn/20190628155249471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
可以发现，将one-hot作为pos信息拼接到x_i中，等同于位置矩阵e_i + a_i ，所以e_i是加到a_i中，而不是拼接。上图中的W_p是可以通过学习得到的，但是google团队说这样的效果并没有很好，他们采用了手动设置一种产生pos的方法，<img src="https://img-blog.csdnimg.cn/20190628155558131.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="decoder_30"></a>decoder</h4>
<p>之前一直没理解为啥decoder可以并行输入，因为看模型decoder的输入需要encoder的embedding和上一步decoder的输出(开头)，既然需要前一步的输出那怎么可能并行呢？</p>
<blockquote>
<p>参考 https://www.zhihu.com/question/307197229</p>
</blockquote>
<p>看了一个知乎的回答，应该算是明白了（不知道理解的对不对）。在训练过程中，decoder直接给的是ground truth的词的embedding，所以在需要前一步的输出的时候，用已知的ground truth embedding代替前一步的输出，这样就能够每步并行，不需要等待前一步的输出。但是在测试的时候，因为没有ground truth embedding做参考，就只能等待前一步的输出，不能够并行。<br>
最后po上transformer完整模型：<br>
<img src="https://img-blog.csdnimg.cn/20190628162821688.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMTQ0MDM2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>
Norm用的是Layer Normalization，与Batch Normalization的不同在于：<br>
batch norm 是对单个神经元的batch个数据上的一种纵向规则化。layer norm是在一层的神经元上进行规则化。batch norm会受到数据相差很大的影响，适用于mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小 mini-batch 场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。但是当数据输入时不同特征时，LN处理可能会降低模型的表达能力。</p>
<blockquote>
<p>参考：http://www.sohu.com/a/220228574_717210  说的十分详细。<br>
自己也把这个博客做了个收藏，以免丢失。</p>
</blockquote>
<p>在decoder中的第一个Masked Multi-head attention layer中，Masked的意思是将一部分输入的信息盖住，即置零。怎么理解呢，就是因为是并行输入的，而前面的单词不应该能够知道后面的单词的信息，所以输入为第i个token时，只能使用i之前的tokens的信息作为他transform的信息，而i后面的信息在i使用时要置零。注意decoder中间层self-attention的输入，K和V来自encoder的输出，而Q来自上一位置的decoder的输出。</p>
</div>
</body>

</html>
